<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Fast Optimization Methods and Application to Data Science and Nonlinear Partial Differential Equations</AwardTitle>
<AwardEffectiveDate>08/01/2020</AwardEffectiveDate>
<AwardExpirationDate>07/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>249999.00</AwardTotalIntnAmount>
<AwardAmount>249999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yuliya Gorb</SignBlockName>
</ProgramOfficer>
<AbstractNarration>This projects incorporates several recent developments in optimization methods and nonlinear multigrid methods to provide a new technique to improve the computational efficiency of practical applications. Successful integration of our fast optimization methods will open a wide new area of applications ranging from numerical solution of partial differential equations to optimization methods for large-scale machine learning. Social media such as Facebook and GitHub  will be used to disseminate basics on applied and computational mathematics and promote the research to a wider audience in both academia and industry, as well as increase the public awareness of how computational mathematics help the advancement of research in other physical and data sciences. This project will provide training opportunities for graduate students.&lt;br/&gt;&lt;br/&gt;The project focuses on a particular nonlinear multigrid method, the fast subspace descent (FASD) method, for solving optimization problems arising from various applications such as numerical solution of partial differential equations and data science problems. For example, the nonlinear multigrid methods to be studied can address the challenging problems in engineering applications including gradient flow in phase field models, Poisson-Boltzmann equation in math biology, and convex composite optimization problems in data science. Acceleration has been one of the most productive ideas in modern optimization theory. This framework brings more insight and mathematical tools for the design and analysis of old and new optimization methods, especially the accelerated gradient descent methods. Another important aspect of this project will be the rigorous theoretical foundation for a large class of optimization methods.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/28/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/28/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>2012465</AwardID>
<Investigator>
<FirstName>Long</FirstName>
<LastName>Chen</LastName>
<EmailAddress>chenlong@math.uci.edu</EmailAddress>
<StartDate>07/28/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Irvine</Name>
<CityName>Irvine</CityName>
<ZipCode>926173213</ZipCode>
<PhoneNumber>9498247295</PhoneNumber>
<StreetAddress>141 Innovation Drive, Ste 250</StreetAddress>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
</Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
</Award>
</rootTag>
