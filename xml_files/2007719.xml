<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Deep Variational Data Compression</AwardTitle>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>425000.00</AwardTotalIntnAmount>
<AwardAmount>425000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rebecca Hwa</SignBlockName>
</ProgramOfficer>
<AbstractNarration>The internet and the world’s IT systems could not exist without data compression. From the efficient storage of large business databases to massive datasets collected by the Large Hadron Collider to video streaming—compression is a tool of fundamental importance that enables many of the systems our societies have come to depend on. It is estimated that by 2021, compressed video data alone will account for over 80% of internet traffic (an estimate made before COVID-19). Any gains that can be made in video coding efficiency will have a dramatic societal impact. Over the past few years, it has become clear that neural networks can significantly improve classical compression methods in terms of how they trade off data quality losses for file size. Besides this, neural compression methods also have other benefits: they can be fine-tuned to specific data modalities (e.g., medical images), do not show the common block-coding visual artifacts, and can be ‘supervised’ to allocate more attention to specific features of interest. This award contributes to better compression algorithms by improving video coding, enabling faster data transfer between machine learning systems, and improving the modularity of neural codec design, potentially impacting a wide range of applications.&lt;br/&gt;&lt;br/&gt;This project draws on deep latent variable modeling and promotes several new ideas for neural data compression: (i) hierarchical generative video coding; (ii) supervised compression, and (iii) plug and play compression of trained generative models. Part (i) proposes to combine normalizing flows with sequential variational autoencoders to predict future frames with higher confidence and shorter expected code lengths. Part (ii) leverages the ability of deep neural networks to be trained towards multiple tasks, such as data reconstruction and classification. Part (iii) describes a fundamentally new approach that decouples discretization from training, and that instead performs discretization and entropy coding jointly. The algorithm takes posterior uncertainties into account to allocate more bits to the features that are most important to reconstruct a given input data point while assigning fewer bits to features where some quantization error can be tolerated.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/02/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/15/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>2007719</AwardID>
<Investigator>
<FirstName>Stephan</FirstName>
<LastName>Mandt</LastName>
<EmailAddress>mandt@uci.edu</EmailAddress>
<StartDate>09/02/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Irvine</Name>
<CityName>Irvine</CityName>
<ZipCode>926173213</ZipCode>
<PhoneNumber>9498247295</PhoneNumber>
<StreetAddress>141 Innovation Drive, Ste 250</StreetAddress>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
</Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
</Appropriation>
</Award>
</rootTag>
