<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>AF: RI: Small: Computationally Efficient Approximation of Stationary Points in Convex and Min-Max Optimization</AwardTitle>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>09/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>350000.00</AwardTotalIntnAmount>
<AwardAmount>350000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>A. Funda Ergun</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Optimization permeates almost every aspect of life, from natural selection and evolution to technological and economic development. Within modern data science, optimization algorithms are the core engine for finding patterns in the data, creating models that explain and mimic them, and making predictions. The primary goal of this project is to advance the theoretical foundations of optimization and leverage the obtained insights to develop new algorithms that are broadly applicable, adaptive to different data models, and scalable, so that they can be applied to the ever-more ambitious data-science applications. One of the guiding principles for the development of theoretical frameworks in this project are parallels between optimization algorithms and laws, such as the principle of least action, governing the behavior of physical systems. &lt;br/&gt;&lt;br/&gt;More concretely, the goal of this project is to further the understanding of how fast it is possible for optimization algorithms to converge to stationary points, defined as the points with small gradient norms. In convex optimization, one of the most fundamental facts is that every stationary point is also a global function minimum. However, the problem of efficiently computing near-stationary points is quite different from the problem of efficiently approximating the function minima, and methods that exhibit optimal convergence rates under one of the criteria do not in general exhibit optimal convergence rates under both. In particular, Nesterov?s accelerated gradient method is iteration-complexity-optimal in terms of minimizing smooth (gradient-Lipschitz) convex functions, but suboptimal in terms of finding their near-stationary points. While the complexity of minimizing convex functions is well-understood, much less is known about the complexity of finding near-stationary points. This troubling gap in understanding causes severe algorithmic limitations not only for general-purpose optimization algorithms, but also in a number of application areas. The primary focus of this project is to close this gap by developing a general framework for the analysis of convergence to stationary points in convex optimization and its generalizations, leveraging technical tools from dynamical systems, monotone-operator theory, and fixed-point theory.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/31/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/31/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>2007757</AwardID>
<Investigator>
<FirstName>Jelena</FirstName>
<LastName>Diakonikolas</LastName>
<EmailAddress>jdiakonikola@wisc.edu</EmailAddress>
<StartDate>07/31/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Wisconsin-Madison</Name>
<CityName>MADISON</CityName>
<ZipCode>537151218</ZipCode>
<PhoneNumber>6082623822</PhoneNumber>
<StreetAddress>21 North Park Street</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<StateCode>WI</StateCode>
</Institution>
<ProgramElement>
<Code>2878</Code>
<Text>Special Projects - CCF</Text>
</ProgramElement>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7926</Code>
<Text>ALGORITHMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
</Award>
</rootTag>
